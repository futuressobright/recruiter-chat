import os
import logging
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
from flask import Flask, render_template, request, jsonify
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from document_utils import load_and_chunk_documents

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Configuration
PORT = int(os.environ.get('PORT', 5001))
HOST = '0.0.0.0'
DOCUMENTS_DIR = 'documents'
QA_MODEL_NAME = 'deepset/roberta-base-squad2'
EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'
CHUNK_SIZE = 200
CHUNK_OVERLAP = 50
MAX_LENGTH = 512
SIMILARITY_THRESHOLD = 2.0  # Increased threshold

# Device setup
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# Load models
tokenizer = AutoTokenizer.from_pretrained(QA_MODEL_NAME)
model = AutoModelForQuestionAnswering.from_pretrained(QA_MODEL_NAME).to(device)
qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer, device=device)
embedding_model = SentenceTransformer(EMBEDDING_MODEL).to(device)

# Load and chunk documents
document_chunks = load_and_chunk_documents(DOCUMENTS_DIR, CHUNK_SIZE, CHUNK_OVERLAP)
logger.info(f"Loaded {len(document_chunks)} chunks")
for i, chunk in enumerate(document_chunks[:5]):
    logger.debug(f"Chunk {i}: {chunk[:100]}...")

chunk_embeddings = embedding_model.encode(document_chunks)

# Create FAISS index
index = faiss.IndexFlatL2(chunk_embeddings.shape[1])
index.add(chunk_embeddings)

def get_answer(question, context):
    result = qa_pipeline(question=question, context=context)
    logger.debug(f"QA result: {result}")
    if result['score'] < 0.1:
        return "I'm sorry, I don't have enough information to answer that question confidently."
    return result['answer']

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    try:
        user_message = request.json['message']
        logger.info(f"Received message: {user_message}")

        # Retrieve relevant document chunks
        query_embedding = embedding_model.encode([user_message])
        distances, indices = index.search(query_embedding, k=3)
        logger.debug(f"Retrieved chunks: {indices[0]}, distances: {distances[0]}")
        
        relevant_chunks = [document_chunks[i] for i, dist in zip(indices[0], distances[0]) if dist < SIMILARITY_THRESHOLD]
        logger.info(f"Found {len(relevant_chunks)} relevant chunks")

        if not relevant_chunks:
            return jsonify({'response': "I'm sorry, I couldn't find any relevant information to answer your question."})

        # Combine relevant chunks into a single context
        context = " ".join(relevant_chunks)
        logger.debug(f"Combined context: {context[:200]}...")

        # Get answer using the QA pipeline
        answer = get_answer(user_message, context)
        logger.info(f"Generated answer: {answer}")

        return jsonify({'response': answer})
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    logger.info(f"Server starting on http://{HOST}:{PORT}")
    app.run(host=HOST, port=PORT, debug=True)
